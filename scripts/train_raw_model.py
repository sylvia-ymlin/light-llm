#!/usr/bin/env python3
"""
è®­ç»ƒåŸå§‹æ–‡æœ¬æ¨¡å‹ï¼Œå®Œå…¨ç»•è¿‡æ ¼å¼åŒ–æ ‡è®°
"""

import torch
import torch.nn.functional as F
from pathlib import Path
from tqdm import tqdm

from llm_scratch.model.base import GPTModern
from llm_scratch.data.tokenizers import ByteTokenizer

def create_raw_training_data():
    """åˆ›å»ºåŸå§‹æ–‡æœ¬è®­ç»ƒæ•°æ®"""
    
    # ç®€å•çš„è‹±æ–‡å¥å­ï¼Œæ²¡æœ‰ä»»ä½•ç‰¹æ®Šæ ‡è®°
    texts = [
        "Hello world, this is a test.",
        "Good morning, how are you today?",
        "The weather is nice and sunny.",
        "I enjoy reading books about science.",
        "Machine learning is very interesting.",
        "Python is a powerful programming language.",
        "Thank you for your help today.",
        "Have a wonderful day ahead.",
        "Nice to meet you here.",
        "See you later, goodbye.",
        "The quick brown fox jumps over the lazy dog.",
        "Artificial intelligence is changing the world.",
        "Deep learning models can understand text.",
        "Natural language processing is fascinating.",
        "Computer vision helps machines see.",
        "Data science combines statistics and programming.",
        "Software engineering requires careful planning.",
        "Web development uses many different technologies.",
        "Mobile apps are becoming more popular.",
        "Cloud computing provides scalable solutions.",
    ]
    
    # å°†æ‰€æœ‰æ–‡æœ¬è¿æ¥æˆä¸€ä¸ªé•¿å­—ç¬¦ä¸²ï¼Œç”¨ç©ºæ ¼åˆ†éš”
    combined_text = " ".join(texts)
    
    print(f"Created training text with {len(combined_text)} characters")
    print(f"Sample: '{combined_text[:100]}...'")
    
    return combined_text

def train_raw_model():
    """ç›´æ¥åœ¨åŸå§‹æ–‡æœ¬ä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹"""
    
    print("ğŸ”¤ Training Raw Text Language Model")
    print("=" * 50)
    
    device = torch.device('cpu')
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®
    text = create_raw_training_data()
    tokenizer = ByteTokenizer()
    
    # åˆ†è¯
    tokens = tokenizer.encode(text)
    print(f"Tokenized to {len(tokens)} tokens")
    
    # åˆ›å»ºæ¨¡å‹
    model = GPTModern(
        vocab_size=256,
        block_size=128,
        n_layer=4,
        n_head=4,
        n_embd=128
    ).to(device)
    
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, betas=(0.9, 0.95), weight_decay=0.1)
    
    # è®­ç»ƒå‚æ•°
    block_size = 128
    batch_size = 4
    steps = 200
    
    model.train()
    
    print(f"\nTraining for {steps} steps...")
    
    for step in tqdm(range(steps)):
        # éšæœºé‡‡æ ·æ‰¹æ¬¡
        batch_inputs = []
        batch_targets = []
        
        for _ in range(batch_size):
            # éšæœºé€‰æ‹©èµ·å§‹ä½ç½®
            start_idx = torch.randint(0, len(tokens) - block_size - 1, (1,)).item()
            
            # è¾“å…¥å’Œç›®æ ‡åºåˆ—
            input_seq = tokens[start_idx:start_idx + block_size]
            target_seq = tokens[start_idx + 1:start_idx + block_size + 1]
            
            batch_inputs.append(input_seq)
            batch_targets.append(target_seq)
        
        # è½¬æ¢ä¸ºtensor
        inputs = torch.stack(batch_inputs).to(device)
        targets = torch.stack(batch_targets).to(device)
        
        # å‰å‘ä¼ æ’­
        logits, _, _ = model(inputs)
        
        # è®¡ç®—æŸå¤±
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        # æ‰“å°è¿›åº¦
        if (step + 1) % 50 == 0:
            print(f"Step {step + 1}: loss = {loss.item():.4f}")
    
    # ä¿å­˜æ¨¡å‹
    out_dir = Path("runs/raw_model")
    out_dir.mkdir(parents=True, exist_ok=True)
    
    config = {
        'vocab_size': 256,
        'block_size': 128,
        'n_layer': 4,
        'n_head': 4,
        'n_embd': 128,
        'tokenizer_type': 'byte'
    }
    
    torch.save({
        'model': model.state_dict(),
        'config': config
    }, str(out_dir / 'model_last.pt'))
    
    print(f"\nâœ… Raw model saved to {out_dir}/model_last.pt")
    return str(out_dir / 'model_last.pt')

def test_raw_model(model_path):
    """æµ‹è¯•åŸå§‹æ–‡æœ¬æ¨¡å‹"""
    
    print("\nğŸ§ª Testing Raw Text Model")
    print("=" * 35)
    
    device = torch.device('cpu')
    
    # åŠ è½½æ¨¡å‹
    ckpt = torch.load(model_path, map_location=device)
    config = ckpt['config']
    
    model = GPTModern(
        vocab_size=config['vocab_size'],
        block_size=config['block_size'],
        n_layer=config['n_layer'],
        n_head=config['n_head'],
        n_embd=config['n_embd']
    ).to(device)
    
    model.load_state_dict(ckpt['model'])
    model.eval()
    
    tokenizer = ByteTokenizer()
    
    # æµ‹è¯•ä¸åŒçš„prompts
    test_prompts = [
        "Hello",
        "Good morning",
        "The weather",
        "Machine learning",
        "Thank you"
    ]
    
    print("Generation results:")
    for prompt in test_prompts:
        input_ids = tokenizer.encode(prompt).unsqueeze(0).to(device)
        
        with torch.no_grad():
            output = model.generate(
                input_ids,
                max_new_tokens=20,
                temperature=0.7,
                top_k=20
            )
        
        generated_text = tokenizer.decode(output[0].tolist())
        print(f"  '{prompt}' â†’ '{generated_text}'")
    
    # åˆ†ææ¨¡å‹æ€§èƒ½
    print(f"\nğŸ“Š Performance Analysis:")
    
    test_input = tokenizer.encode("Hello world").unsqueeze(0).to(device)
    with torch.no_grad():
        logits, _, _ = model(test_input)
        probs = torch.softmax(logits[0, -1, :], dim=-1)
    
    entropy = -(probs * torch.log(probs + 1e-10)).sum().item()
    max_prob = probs.max().item()
    
    print(f"  Entropy: {entropy:.4f}")
    print(f"  Max probability: {max_prob:.6f}")
    print(f"  Perplexity: {torch.exp(torch.tensor(entropy)):.2f}")

if __name__ == "__main__":
    # è®­ç»ƒåŸå§‹æ–‡æœ¬æ¨¡å‹
    model_path = train_raw_model()
    
    # æµ‹è¯•æ¨¡å‹
    test_raw_model(model_path)