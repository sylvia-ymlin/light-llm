#!/usr/bin/env python3
"""
è®­ç»ƒä¸€ä¸ªå¹²å‡€çš„è¯­è¨€æ¨¡å‹ï¼Œä¸ä½¿ç”¨ç‰¹æ®Šæ ¼å¼æ ‡è®°
"""

import torch
from llm_scratch.training.sft import train_sft
from llm_scratch.model.base import GPTModern
from llm_scratch.data.tokenizers import ByteTokenizer

def create_clean_training_data():
    """åˆ›å»ºå¹²å‡€çš„è®­ç»ƒæ•°æ®ï¼Œä¸ä½¿ç”¨ç‰¹æ®Šæ ‡è®°"""
    
    # ç®€å•çš„å¥å­è¡¥å…¨æ•°æ®
    sentences = [
        "Hello world, this is a test of our language model.",
        "The quick brown fox jumps over the lazy dog.",
        "Machine learning is a subset of artificial intelligence.",
        "Python is a popular programming language.",
        "Natural language processing helps computers understand text.",
        "Deep learning models can generate human-like text.",
        "The weather is nice today.",
        "I enjoy reading books about science.",
        "Cooking is both an art and a science.",
        "Music has the power to evoke emotions.",
        "Travel broadens the mind.",
        "Education is the key to growth.",
        "Exercise is important for health.",
        "Innovation drives progress.",
        "Creativity helps solve problems.",
        "Good morning, how are you today?",
        "Thank you for your help.",
        "Nice to meet you.",
        "Have a great day!",
        "See you later.",
    ]
    
    # åˆ›å»ºç®€å•çš„è¡¥å…¨ä»»åŠ¡ï¼Œä¸ä½¿ç”¨ç‰¹æ®Šæ ¼å¼
    training_pairs = []
    
    for sentence in sentences:
        words = sentence.split()
        # åˆ›å»ºä¸åŒé•¿åº¦çš„è¡¥å…¨ä»»åŠ¡
        for i in range(2, min(len(words), 8)):  # é™åˆ¶é•¿åº¦é¿å…è¿‡é•¿
            prompt = " ".join(words[:i])
            response = " ".join(words[i:])
            # ç›´æ¥ä½¿ç”¨æ–‡æœ¬ï¼Œä¸æ·»åŠ ç‰¹æ®Šæ ‡è®°
            training_pairs.append((prompt, response))
    
    # æ·»åŠ ä¸€äº›å¯¹è¯æ•°æ®
    conversations = [
        ("Hello", "Hi there!"),
        ("How are you", "I am doing well, thank you."),
        ("What is your name", "I am an AI assistant."),
        ("Good morning", "Good morning to you too!"),
        ("Thank you", "You are welcome."),
        ("Nice weather", "Yes, it is a beautiful day."),
        ("See you later", "Goodbye, have a nice day!"),
    ]
    
    training_pairs.extend(conversations)
    
    # é‡å¤æ•°æ®å¢åŠ è®­ç»ƒé‡
    training_pairs = training_pairs * 15
    
    print(f"Created {len(training_pairs)} clean training pairs")
    return training_pairs

def train_clean_model():
    """è®­ç»ƒå¹²å‡€çš„æ¨¡å‹"""
    
    print("ğŸ§¹ Training Clean Language Model")
    print("=" * 50)
    
    # åˆ›å»ºå¹²å‡€çš„è®­ç»ƒæ•°æ®
    training_data = create_clean_training_data()
    
    # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
    print("Sample training data:")
    for i in range(3):
        prompt, response = training_data[i]
        print(f"  '{prompt}' â†’ '{response}'")
    print()
    
    # è®­ç»ƒæ¨¡å‹
    train_sft(
        items=training_data,
        out_dir="runs/clean_model",
        steps=150,
        batch_size=8,
        block_size=128,  # è¾ƒçŸ­çš„åºåˆ—
        n_layer=4,
        n_head=4,
        n_embd=128,
        lr=5e-4,  # ç¨é«˜çš„å­¦ä¹ ç‡
        device='cpu'
    )
    
    print("âœ… Clean model training completed!")
    return "runs/clean_model/model_last.pt"

def test_clean_model(model_path):
    """æµ‹è¯•å¹²å‡€çš„æ¨¡å‹"""
    
    print("\nğŸ§ª Testing Clean Model")
    print("=" * 30)
    
    # åŠ è½½æ¨¡å‹
    ckpt = torch.load(model_path, map_location='cpu')
    config = ckpt['config']
    
    model = GPTModern(
        vocab_size=config['vocab_size'],
        block_size=config['block_size'],
        n_layer=config['n_layer'],
        n_head=config['n_head'],
        n_embd=config['n_embd']
    )
    model.load_state_dict(ckpt['model'])
    model.eval()
    
    tokenizer = ByteTokenizer()
    
    # æµ‹è¯•å¤šä¸ªprompt
    test_prompts = [
        "Hello",
        "Good morning",
        "How are",
        "The weather is",
        "Machine learning",
        "Thank you"
    ]
    
    print("Generation results:")
    for prompt in test_prompts:
        input_ids = tokenizer.encode(prompt).unsqueeze(0)
        
        with torch.no_grad():
            # ä½¿ç”¨è¾ƒä½çš„temperatureè·å¾—æ›´ç¨³å®šçš„è¾“å‡º
            output = model.generate(
                input_ids, 
                max_new_tokens=15, 
                temperature=0.5,  # é™ä½éšæœºæ€§
                top_k=10
            )
            
        generated_text = tokenizer.decode(output[0].tolist())
        print(f"  '{prompt}' â†’ '{generated_text}'")

def analyze_model_performance(model_path):
    """åˆ†ææ¨¡å‹æ€§èƒ½"""
    
    print("\nğŸ“Š Model Performance Analysis")
    print("=" * 40)
    
    # åŠ è½½æ¨¡å‹
    ckpt = torch.load(model_path, map_location='cpu')
    config = ckpt['config']
    
    model = GPTModern(
        vocab_size=config['vocab_size'],
        block_size=config['block_size'],
        n_layer=config['n_layer'],
        n_head=config['n_head'],
        n_embd=config['n_embd']
    )
    model.load_state_dict(ckpt['model'])
    model.eval()
    
    tokenizer = ByteTokenizer()
    
    # åˆ†æè¾“å‡ºåˆ†å¸ƒ
    test_prompt = "Hello world"
    input_ids = tokenizer.encode(test_prompt).unsqueeze(0)
    
    with torch.no_grad():
        logits, _, _ = model(input_ids)
        probs = torch.softmax(logits[0, -1, :], dim=-1)
        
    entropy = -(probs * torch.log(probs + 1e-10)).sum().item()
    max_prob = probs.max().item()
    
    print(f"Output distribution analysis:")
    print(f"  Entropy: {entropy:.4f} (lower is more confident)")
    print(f"  Max probability: {max_prob:.6f}")
    print(f"  Perplexity: {torch.exp(torch.tensor(entropy)):.2f}")
    
    # æ˜¾ç¤ºæœ€å¯èƒ½çš„tokens
    top_k = 5
    top_probs, top_indices = torch.topk(probs, top_k)
    print(f"\nTop {top_k} most likely next tokens:")
    for i in range(top_k):
        token_id = top_indices[i].item()
        prob = top_probs[i].item()
        char = chr(token_id) if 32 <= token_id <= 126 else f'[{token_id}]'
        print(f"  '{char}': {prob:.6f}")

if __name__ == "__main__":
    # è®­ç»ƒå¹²å‡€çš„æ¨¡å‹
    model_path = train_clean_model()
    
    # æµ‹è¯•æ¨¡å‹
    test_clean_model(model_path)
    
    # åˆ†ææ€§èƒ½
    analyze_model_performance(model_path)